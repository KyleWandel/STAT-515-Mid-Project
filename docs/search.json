[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 515 Mid Project",
    "section": "",
    "text": "The Redesign Project has two main objectives:"
  },
  {
    "objectID": "index.html#instructions",
    "href": "index.html#instructions",
    "title": "STAT 515 Mid Project",
    "section": "Instructions",
    "text": "Instructions\nYou are required to discover a visualization from the web or another source and create several redesigns of the original visualization. This process should help you identify unique patterns or information that were not initially evident.\nAdditionally, you are expected to compare and analyze the strengths and weaknesses of both the original plot and your redesigns, considering the context.\nThe redesign project includes, updating your website with the redesigned project, accompanied by a 10minute oral video presentation embedded on your website."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am currently a Manager of industry analysis and business intelligence for the Consumer Technology Association (CTA). My primary role for CTA is to oversee and manage our forecast data program. Using both internal and external data sets, I create various reports and analysis to help the industry in a variety of ways. I would consider myself an expert in how the technology industry/market is performing from a sales perpesctive and how emerging technologies and trends are impacting the industry.\nI live in Fairfax, VA with my wonderful wife Courtney and our 3 incredibly cuddly and anxious animals. We have two dogs, Buddy and Maximus and a cat named Bentley.\nI am an avid sports enthusiast and probably know the sports world more than I do the technology industry. My favorite sport to watch, talk, analyze and write about is football, but I really love all sports.\nAnalyzing and tracking data has always been a passion of mine and I incorporate it in everything that I do. From pacing and tracking my times training for triathlons to arguing about sports, I believe data is the key to everything."
  },
  {
    "objectID": "new_graphs.html",
    "href": "new_graphs.html",
    "title": "Redesigned Visuals",
    "section": "",
    "text": "In order to make the original dataset more usable I had to clean it by removing some of the extra rows before reading the file into R. I updated the names for the columns and then I had to pivot the data in order to create the various buckets I needed by murder type.\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"report\")\nsource(\"/Users/kwandel/Documents/Personal Stuff/George Mason University/STAT 515 - Spring 2024/R Theme/hw.R\")\n\ndf &lt;- read.csv(file = \"/Users/kwandel/Documents/Personal Stuff/George Mason University/STAT 515 - Spring 2024/Porject Folder/Mid Project/Murrder_By_Weapon.csv\")\n\n# Original column names\noriginal_colnames &lt;- colnames(df)\n\n# New column names with spaces instead of periods\nnew_colnames &lt;- gsub(\"\\\\.\", \" \", original_colnames)\n\n# Rename the columns\ncolnames(df) &lt;- new_colnames\n\n# Reshape data to long format\ndf_long &lt;- pivot_longer(df, -Year, names_to = \"Offense_Type\", values_to = \"Count\")"
  },
  {
    "objectID": "new_graphs.html#data-cleaning",
    "href": "new_graphs.html#data-cleaning",
    "title": "Redesigned Visuals",
    "section": "",
    "text": "In order to make the original dataset more usable I had to clean it by removing some of the extra rows before reading the file into R. I updated the names for the columns and then I had to pivot the data in order to create the various buckets I needed by murder type.\n\nlibrary(\"tidyverse\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"report\")\nsource(\"/Users/kwandel/Documents/Personal Stuff/George Mason University/STAT 515 - Spring 2024/R Theme/hw.R\")\n\ndf &lt;- read.csv(file = \"/Users/kwandel/Documents/Personal Stuff/George Mason University/STAT 515 - Spring 2024/Porject Folder/Mid Project/Murrder_By_Weapon.csv\")\n\n# Original column names\noriginal_colnames &lt;- colnames(df)\n\n# New column names with spaces instead of periods\nnew_colnames &lt;- gsub(\"\\\\.\", \" \", original_colnames)\n\n# Rename the columns\ncolnames(df) &lt;- new_colnames\n\n# Reshape data to long format\ndf_long &lt;- pivot_longer(df, -Year, names_to = \"Offense_Type\", values_to = \"Count\")"
  },
  {
    "objectID": "new_graphs.html#graph-1",
    "href": "new_graphs.html#graph-1",
    "title": "Redesigned Visuals",
    "section": "Graph 1",
    "text": "Graph 1\nThe first graph is the original graph that was created but I flipped the y-axis and the corresponding fill to the correct orientation. I also decided to add a few more data points to the graph to make it even easier to understand the change occurring.\n\nggplot(df_long[df_long$Offense_Type == \"Firearm\", ], aes(x = Year, y = Count)) +\n  geom_line(color = \"red\") +\n  geom_point(color = \"red\") +\n  geom_area(fill = \"lightcoral\", alpha = 0.5) +\n  geom_text(data = subset(df_long[df_long$Offense_Type == \"Firearm\", ], Year %% 5 == 0), aes(label = Count), vjust = 1, color = \"black\", size = 3) +  # Add labels to data points every 5th point\n  labs(x = \"Year\", y = \"Murders\", title = \"Firearm Murders Over Years\") +\n  hw\n\n\n\n\n\n\n\n\nFixing the y-axis makes the graph much easier to understand and it can be clearly seen that the number of murders caused by firearms has increased since 2005."
  },
  {
    "objectID": "new_graphs.html#graph-2",
    "href": "new_graphs.html#graph-2",
    "title": "Redesigned Visuals",
    "section": "Graph 2",
    "text": "Graph 2\nThis next graph is how I would officially redesign the data and would show it in the article.\n\n# Filter out \"Total Offenses\" category\ndf_filtered &lt;- df_long[df_long$Offense_Type != \"Total Offenses\", ]\n\n# Plotting\nggplot(df_filtered, aes(x = Year, y = Count, color = Offense_Type)) +\n  geom_line() +\n  labs(x = \"Year\", y = \"Murders\", title = \"Murders by Weapon Over Years\") +\n  hw\n\n\n\n\n\n\n\n\nThis graph utilizes the full dataset and looks at the number of murders for each type of offense. By looking at the graph the reader can clearly see that the number of murders using a firearm has increased since 2005. But the reader could also see how many more murders are committed using a firearm compared to other types, which by looking at the graph are decreasing."
  },
  {
    "objectID": "new_graphs.html#references",
    "href": "new_graphs.html#references",
    "title": "Redesigned Visuals",
    "section": "References",
    "text": "References\n\ncite_packages()\n\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - Makowski D, Lüdecke D, Patil I, Thériault R, Ben-Shachar M, Wiernik B (2023). \"Automated Results Reporting as a Practical Tool to Improve Reproducibility and Methodological Best Practices Adoption.\" _CRAN_. &lt;https://easystats.github.io/report/&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - R Core Team (2023). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2022). _stringr: Simple, Consistent Wrappers for Common String Operations_. R package version 1.5.0, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. R package version 1.1.3, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2023). _purrr: Functional Programming Tools_. R package version 1.0.2, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2023). _readr: Read Rectangular Text Data_. R package version 2.1.4, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Vaughan D, Girlich M (2023). _tidyr: Tidy Messy Data_. R package version 1.3.0, &lt;https://CRAN.R-project.org/package=tidyr&gt;."
  },
  {
    "objectID": "original_graph.html",
    "href": "original_graph.html",
    "title": "Original Graph",
    "section": "",
    "text": "Reuters graph - Number of murders by firearm"
  },
  {
    "objectID": "original_graph.html#graph",
    "href": "original_graph.html#graph",
    "title": "Original Graph",
    "section": "",
    "text": "Reuters graph - Number of murders by firearm"
  },
  {
    "objectID": "original_graph.html#analysis-of-graph",
    "href": "original_graph.html#analysis-of-graph",
    "title": "Original Graph",
    "section": "Analysis of Graph",
    "text": "Analysis of Graph\nThe original graph comes from a Reuters article highlighting how murders in Florida have rapidly increased in the 10 years after Florida enacted it’s stand your ground laws in 2005. The graph was criticized so badly that they took it down shortly after publishing it. However, it was up long enough for everyone to see the poorly constructed figure and it is now often used to depict horrible graphics.\nThe data that Reuters used is from Florida’s Department of Law Enforcement and is created by Criminal Justice Analytics Bureau. The report is an annual report that aggregates all crime that occurred in the state of Florida. There are numerous reports created by the bureau, including type of offense, location, and by weapon type, which is where the data from the graph comes from."
  },
  {
    "objectID": "original_graph.html#references",
    "href": "original_graph.html#references",
    "title": "Original Graph",
    "section": "References",
    "text": "References\nRapaport, L. (2017, August 14). Murders surge in Florida in decade after `Stand Your Ground’ law. Reuters. https://www.reuters.com/article/idUSKCN1AU1QK/\nUCR Offense Data. (2022, September 1). Www.fdle.state.fl.us; Criminal Justice Analytics Bureau. https://www.fdle.state.fl.us/CJAB/UCR/Annual-Reports/UCR-Offense-Data"
  },
  {
    "objectID": "Final_KW.html",
    "href": "Final_KW.html",
    "title": "FINAL PROJECT",
    "section": "",
    "text": "Group Number: 2\nAnusha Dusakanti, Kyle Wandel, Sumrah Shakeel"
  },
  {
    "objectID": "Final_KW.html#iii.-limitations",
    "href": "Final_KW.html#iii.-limitations",
    "title": "FINAL PROJECT",
    "section": "iii. Limitations",
    "text": "iii. Limitations\nThere were no major limitations for this analysis but there were a few of things that needed to be done to the dataset in order to clean and make the dataset usable for analysis. First, we had to identify and remove all missing rows from the dataset. Second, we had to transform the response variable to be “0” and “1”. Finally, many of the variables seemed to show a left skew making us question if the variables should be transformed or not. \nThere were only 699 samples for the dataset and although a solid number of samples, more samples would lead to a more predictable conclusion."
  },
  {
    "objectID": "Final_KW.html#iv.-conclusion",
    "href": "Final_KW.html#iv.-conclusion",
    "title": "FINAL PROJECT",
    "section": "iv. Conclusion",
    "text": "iv. Conclusion\nIt seems that a significantly accurate model to predict if a cancerous cell could be malignant using the measurements recorded in this dataset. To test this, we created multiple regression models, a random forest model, and a PCA analysis to understand the variables more thoroughly.  \nIn the PCA test we determined that the dataset’s variance could be explained by simplifying and using only 2 of the 9 predictor variables. We then created multiple logistic regression models and compared them to each other to choose the best one. After variable transformation and selection, we determined the best model would be to use all the variables. The percent chance the model falsely predicted cancer when not was 10/444 (2.2%) and the chance the model falsely predicted not having cancer when there was 11/239 (4.6%). We then decided to try and create an optimal best random forest model, starting with a best fit decision tree model. The created model resulted in an overall accuracy rate of almost 98% with the percent chance the model falsely predicted cancer when not was 6/444 (1.4%) and the chance the model falsely predicted not having cancer when there was 9/239 (3.7%). Based on the two models we created, the best model to use would be the random forest model because of its high accuracy and predictability."
  },
  {
    "objectID": "Final_KW.html#references",
    "href": "Final_KW.html#references",
    "title": "FINAL PROJECT",
    "section": "References",
    "text": "References\n\nWolberg,WIlliam. (1992). Breast Cancer Wisconsin (Original). UCI Machine Learning Repository. https://doi.org/10.24432/C5HP4Z. \nGrolemund, G., & Wickham, H. (2011). “Dates and Times Made Easy with lubridate.” Journal of Statistical Software, 40(3), 1-25. [Online]. Available: https://www.jstatsoft.org/v40/i03/\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). ISLR: Data for an Introduction to Statistical Learning with Applications in R. R package version 1.4. [Online]. Available: https://CRAN.R-project.org/package=ISLR\nKuhn, M. (2008). “Building Predictive Models in R Using the caret Package.” Journal of Statistical Software, 28(5), 1–26. doi:10.18637/jss.v028.i05 [Online]. Available: https://doi.org/10.18637/jss.v028.i05\nLiaw, A., & Wiener, M. (2002). “Classification and Regression by randomForest.” R News, 2(3), 18-22. [Online]. Available: https://CRAN.R-project.org/doc/Rnews/\nMakowski, D., Lüdecke, D., Patil, I., Thériault, R., Ben-Shachar, M., & Wiernik, B. (2023). “Automated Results Reporting as a Practical Tool to Improve Reproducibility and Methodological Best Practices Adoption.” CRAN. [Online]. Available: https://easystats.github.io/report/\nMiller, T. (2020). leaps: Regression Subset Selection. R package version 3.1. [Online]. Available: https://CRAN.R-project.org/package=leaps\nMüller, K., & Wickham, H. (2023). tibble: Simple Data Frames. R package version 3.2.1. [Online]. Available: https://CRAN.R-project.org/package=tibble\nR Core Team (2023). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria. [Online]. Available: https://www.R-project.org/\nRipley, B. (2023). tree: Classification and Regression Trees. R package version 1.0-43. [Online]. Available: https://CRAN.R-project.org/package=tree\nSarkar, D. (2008). Lattice: Multivariate Data Visualization with R. Springer, New York. ISBN 978-0-387-75968-5. [Online]. Available: http://lmdvr.r-forge.r-project.org\nSievert, C. (2020). Interactive Web-Based Data Visualization with R, plotly, and shiny. Chapman and Hall/CRC. ISBN 9781138331457. [Online]. Available: https://plotly-r.com\nWickham, H. (2007). “Reshaping Data with the reshape Package.” Journal of Statistical Software, 21(12), 1-20. [Online]. Available: http://www.jstatsoft.org/v21/i12/\nWickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. ISBN 978-3-319-24277-4. [Online]. Available: https://ggplot2.tidyverse.org\nWickham, H. (2022). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.5.0. [Online]. Available: https://CRAN.R-project.org/package=stringr\nWickham, H. (2023). forcats: Tools for Working with Categorical Variables (Factors). R package version 1.0.0. [Online]. Available: https://CRAN.R-project.org/package=forcats\nWickham, H., et al. (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686 [Online]. Available: https://doi.org/10.21105/joss.01686\nWickham, H., François, R., Henry, L., Müller, K., & Vaughan, D. (2023). dplyr: A Grammar of Data Manipulation. R package version 1.1.3. [Online]. Available: https://CRAN.R-project.org/package=dplyr\nWickham, H., & Henry, L. (2023). purrr: Functional Programming Tools. R package version 1.0.2. [Online]. Available: https://CRAN.R-project.org/package=purrr\nWickham, H., Hester, J., & Bryan, J. (2023). readr: Read Rectangular Text Data. R package version 2.1.4. [Online]. Available: https://CRAN.R-project.org/package=readr\nWickham, H., Vaughan, D., & Girlich, M. (2023). tidyr: Tidy Messy Data. R package version 1.3.0. [Online]. Available: https://CRAN.R-project.org/package=tidyr\nRevelle, W. (2024). psych: Procedures for Psychological, Psychometric, and Personality Research. Northwestern University, Evanston, Illinois. R package version 2.4.3. [Online]. Available: https://CRAN.R-project.org/package=psych"
  },
  {
    "objectID": "Final_Report_Webpage.html",
    "href": "Final_Report_Webpage.html",
    "title": "Final Report",
    "section": "",
    "text": "Attaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ ggplot2   3.5.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\n\n\nAttaching package: 'reshape2'\n\n\nThe following object is masked from 'package:tidyr':\n\n    smiths\n\n\nrandomForest 4.7-1.1\n\nType rfNews() to see new features/changes/bug fixes.\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:psych':\n\n    outlier\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nLoading required package: lattice\n\n\nAttaching package: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead."
  },
  {
    "objectID": "Final_Report_Webpage.html#abstract",
    "href": "Final_Report_Webpage.html#abstract",
    "title": "Final Report",
    "section": "Abstract",
    "text": "Abstract\nFor the final project for our STAT 515 class, we were asked to find a dataset and perform a robust advanced analysis on the data. For this project we chose to use breast cancer screening data from the University of Wisconsin [1]. This study aims to identify patterns and indicators that could potentially predict the signs of malignant cancerous cells. Through meticulous analysis, the paper investigates various aspects, including a deep dive into the predictor variables and their relationship to each other and the response variable, the creation of various statistical models to predict cancerous cells and then finally a comparison of models to choose the best one. By scrutinizing these factors, this research hopes to make discovering breast cancer in patients easier and earlier."
  },
  {
    "objectID": "Final_Report_Webpage.html#i.-introduction",
    "href": "Final_Report_Webpage.html#i.-introduction",
    "title": "Final Report",
    "section": "i. Introduction",
    "text": "i. Introduction\nBreast cancer is one of the most common cancers in the world. While the pharmaceutical industry has invested quite a bit in trying to find a definitive cure to this cancer, it still raises the need for more analysis to be done on breast cancer data so that a cancerous tumor can be caught in the early stages. We wanted to see if it was possible to identify the potential emergence of breast cancer in women based on different features of the tumor cells. This dataset seemed like a good fit since it has 9 predictor variables, and the outcome variable would indicate whether the tumor can be classified as malignant (cancerous) or benign (non-cancerous). \nThe dataset we choose to perform this research is from Dr. William Wolberg and his clinical studies from 1989 to 1991. This dataset is very well known and highly integrable due to the amount of research conducted using this data. Below is a list and description of the 9 predictor variables and the 1 response variable (benormal).  \n\nclumpthickness: (1-10). Benign cells tend to be grouped in monolayers, while cancerous cells are often grouped in multilayers. \nuniformcellsize (1-10). Cancer cells tend to vary in size and shape. \nuniformcellshape (1-10). Cancer cells tend to vary in shape and size. \nmargadhesion: (1-10). Normal cells tend to stick together, while cancer cells tend to lose this ability, so the loss of adhesion is a sign of malignancy. \nepithelial: (1-10). It is related to the uniformity mentioned above. Epithelial cells that are significantly enlarged may be malignant. \nbarenuclei: (1-10). This term is used for nuclei not surrounded by cytoplasm (the rest of the cell). Those are typically seen in benign tumors. \nblandchromatin: (1-10). Describes a uniform “texture” of the nucleus seen in benign cells. In cancer cells, the chromatin tends to be more coarse and to form clumps. \nnormalnucleoli: (1-10). Nucleoli are small structures seen in the nucleus. In normal cells, the nucleolus is usually very small, if visible. The nucleoli become more prominent in cancer cells, and sometimes there are multiple. \nmitoses: (1-10). Cancer is essentially a disease of uncontrolled mitosis. \nbenormal: (2 or 4). Benign (non-cancerous) or malignant (cancerous) lump in a breast. ii. Materials and Methods\n\nThe University of Wisconsin breast cancer data from William Wolberg has 699 observations and 10 variables, the first variable represents the ID of the sample and the last column “benornal” represents the classification/response variable (for benign, 4 for malignant).\n\n\n'data.frame':   699 obs. of  11 variables:\n $ id              : int  1000025 1002945 1015425 1016277 1017023 1017122 1018099 1018561 1033078 1033078 ...\n $ clumpthickness  : int  5 5 3 6 4 8 1 2 2 4 ...\n $ uniformcellsize : int  1 4 1 8 1 10 1 1 1 2 ...\n $ uniformcellshape: int  1 4 1 8 1 10 1 2 1 1 ...\n $ margadhesion    : int  1 5 1 1 3 8 1 1 1 1 ...\n $ epithelial      : int  2 7 2 3 2 7 2 2 2 2 ...\n $ barenuclei      : chr  \"1\" \"10\" \"2\" \"4\" ...\n $ blandchromatin  : int  3 3 3 3 3 9 3 3 1 2 ...\n $ normalnucleoli  : int  1 2 1 7 1 7 1 1 1 1 ...\n $ mitoses         : int  1 1 1 1 1 1 1 1 5 1 ...\n $ benormal        : int  2 2 2 2 2 4 2 2 2 2 ...\n\n\nUsing this dataset we wanted to try and answer the following research questions:\nQuestion 1: What are the variables displaying significant correlation?\nQuestion 2: How does Principal Component Analysis contribute to understanding the variance explained by principal components in our dataset?\nQuestion 3: Can a logistic regression model be created to accurately predict malignant breast cancer cells?\nQuestion 4: Can a random forest model be created to accurately predict the likelihood of a cell being malignant?\nTo make the dataset ready for analysis we removed the ID column, checked and removed all rows with missing data changed he response variable values to malignant (4) = 1 and benign (2) = 0.\n\n\nWarning: NAs introduced by coercion\n\n\n  clumpthickness  uniformcellsize uniformcellshape     margadhesion \n               0                0                0                0 \n      epithelial       barenuclei   blandchromatin   normalnucleoli \n               0               16                0                0 \n         mitoses         benormal \n               0                0 \n\n\n'data.frame':   683 obs. of  10 variables:\n $ clumpthickness  : int  5 5 3 6 4 8 1 2 2 4 ...\n $ uniformcellsize : int  1 4 1 8 1 10 1 1 1 2 ...\n $ uniformcellshape: int  1 4 1 8 1 10 1 2 1 1 ...\n $ margadhesion    : int  1 5 1 1 3 8 1 1 1 1 ...\n $ epithelial      : int  2 7 2 3 2 7 2 2 2 2 ...\n $ barenuclei      : int  1 10 2 4 1 10 10 1 1 1 ...\n $ blandchromatin  : int  3 3 3 3 3 9 3 3 1 2 ...\n $ normalnucleoli  : int  1 2 1 7 1 7 1 1 1 1 ...\n $ mitoses         : int  1 1 1 1 1 1 1 1 5 1 ...\n $ benormal        : int  2 2 2 2 2 4 2 2 2 2 ...\n - attr(*, \"na.action\")= 'omit' Named int [1:16] 24 41 140 146 159 165 236 250 276 293 ...\n  ..- attr(*, \"names\")= chr [1:16] \"24\" \"41\" \"140\" \"146\" ...\n\n\n clumpthickness   uniformcellsize  uniformcellshape  margadhesion  \n Min.   : 1.000   Min.   : 1.000   Min.   : 1.000   Min.   : 1.00  \n 1st Qu.: 2.000   1st Qu.: 1.000   1st Qu.: 1.000   1st Qu.: 1.00  \n Median : 4.000   Median : 1.000   Median : 1.000   Median : 1.00  \n Mean   : 4.442   Mean   : 3.151   Mean   : 3.215   Mean   : 2.83  \n 3rd Qu.: 6.000   3rd Qu.: 5.000   3rd Qu.: 5.000   3rd Qu.: 4.00  \n Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :10.00  \n   epithelial       barenuclei     blandchromatin   normalnucleoli \n Min.   : 1.000   Min.   : 1.000   Min.   : 1.000   Min.   : 1.00  \n 1st Qu.: 2.000   1st Qu.: 1.000   1st Qu.: 2.000   1st Qu.: 1.00  \n Median : 2.000   Median : 1.000   Median : 3.000   Median : 1.00  \n Mean   : 3.234   Mean   : 3.545   Mean   : 3.445   Mean   : 2.87  \n 3rd Qu.: 4.000   3rd Qu.: 6.000   3rd Qu.: 5.000   3rd Qu.: 4.00  \n Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :10.00  \n    mitoses          benormal     \n Min.   : 1.000   Min.   :0.0000  \n 1st Qu.: 1.000   1st Qu.:0.0000  \n Median : 1.000   Median :0.0000  \n Mean   : 1.603   Mean   :0.3499  \n 3rd Qu.: 1.000   3rd Qu.:1.0000  \n Max.   :10.000   Max.   :1.0000  \n\n\n\n  0   1 \n444 239 \n\n\nAfter cleaning the dataset, we looked at a summary of the statistics for each variable. For our sample there are 444 records that are identified as not being malignant (=0) and 239 records that are identified as being malignant (=1).\nWe next wanted to identify if there were any patterns amongst the predictor variables in the dataset. First we looked at the correlations, histrograms and scatterplots of the variables using the pairs.panel() function.\n\n\n\n\n\n\n\n\n\nSome of the variables showed correlations to each other, but none were deemed significant by the corr.test() function. But one thing we did notice was that many of the variables exhibited a right skew with their means larger than their medians. We could potentially log() the variables to make them more uniform."
  },
  {
    "objectID": "Final_Report_Webpage.html#ii.-materials-and-methods",
    "href": "Final_Report_Webpage.html#ii.-materials-and-methods",
    "title": "Final Report",
    "section": "ii. Materials and Methods",
    "text": "ii. Materials and Methods\n\nPrincipal Component Analysis\nWe mentioned earlier that some of our variables were correlated but none were significantly correlated to each other. Also, in some of our other models, some of the variables were not significantly correlated to predicting the response variable. To examine this further, we did a principal component analysis on the predictor variables too see if reducing the number of variables of a data set naturally comes at the expense of accuracy while not losing too much accuracy.\nWe first took a look a the mean and variances of the variables in the dataset to see if the variables should be scaled or not. Below are the means an variances of the variables:\n\n\n  clumpthickness  uniformcellsize uniformcellshape     margadhesion \n        4.442167         3.150805         3.215227         2.830161 \n      epithelial       barenuclei   blandchromatin   normalnucleoli \n        3.234261         3.544656         3.445095         2.869693 \n         mitoses \n        1.603221 \n\n\n  clumpthickness  uniformcellsize uniformcellshape     margadhesion \n        7.956694         9.395113         8.931615         8.205717 \n      epithelial       barenuclei   blandchromatin   normalnucleoli \n        4.942109        13.277695         6.001013         9.318772 \n         mitoses \n        3.002160 \n\n\n  clumpthickness  uniformcellsize uniformcellshape     margadhesion \n        2.820761         3.065145         2.988581         2.864562 \n      epithelial       barenuclei   blandchromatin   normalnucleoli \n        2.223085         3.643857         2.449697         3.052666 \n         mitoses \n        1.732674 \n\n\nBased on these results the following variables exhibit some significant diversity across the dataset; clumpthickness, uniformcellsize, uniformcellshape, margadhesion, barenuclei and normalnucleoli. The following variables showed some moderate variability across the dataset; epithelial, blandchromatin, and mitoses. Because of these results it would be best to scale the variables for analysis.\nAfter running a PCA analysis found that most of the variables are grouped indicating that there are similar observations based on the variable values. There are not too many outliers to indicate any unusual observations.\n\n\n\n\n\n\n\n\n\nLooking at the bigplot mitoses has the largest contribution when looking at the variables to the corresponding principal component. All of the variables, except for mitoses are positively correlated. Most of the variables are closer to all the other variables except for mitoses, further indicating that they are strongly influenced by these corresponding variables. Looking at the axis’ of the graph PC1 explains the most variance in the data as it has the longer axis than PC2.\nThe following graph is the scree plot for the PCA which is used to provide a simple yet effective way to see balance and the trade-off between retaining information and reducing dimensionality in your data.\n\n\n[1] 5.89949935 0.77594689 0.53925224 0.45962745 0.38027583 0.30187645 0.29440271\n[8] 0.26073586 0.08838322\n\n\n[1] 0.655499928 0.086216321 0.059916916 0.051069717 0.042252870 0.033541828\n[7] 0.032711413 0.028970651 0.009820358\n\n\n\n\n\n\n\n\n\nLooking at this graph we can easily identify that there is a large drop off-point after the first PCA, indicating that adding an additional principal component does not significantly add value at explaining the variability in the data.\nThe following graph shows cumulative proportion of variance that each principal components adds.\n\n\n\n\n\n\n\n\n\nLooking at this graph it seems that 4 principal components would be needed to explain over 80% of the variance.\n\n\nLogistic Regression Model\nWe decided to develop a Logistic regression model to see if we could develop a model that could be used to predict if a cell was cancerous or not. Because our outcome can only be one of two things (cell is malignant or benign) we should be using a classification model and logistic regression is a simple model which is much easier to set up and train initially than other machine learning models.\nFor the first model we used the cleaned dataset and all of the variables.\n\n\n\nCall:\nglm(formula = benormal ~ ., family = binomial, data = df_clean)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -10.10394    1.17488  -8.600  &lt; 2e-16 ***\nclumpthickness     0.53501    0.14202   3.767 0.000165 ***\nuniformcellsize   -0.00628    0.20908  -0.030 0.976039    \nuniformcellshape   0.32271    0.23060   1.399 0.161688    \nmargadhesion       0.33064    0.12345   2.678 0.007400 ** \nepithelial         0.09663    0.15659   0.617 0.537159    \nbarenuclei         0.38303    0.09384   4.082 4.47e-05 ***\nblandchromatin     0.44719    0.17138   2.609 0.009073 ** \nnormalnucleoli     0.21303    0.11287   1.887 0.059115 .  \nmitoses            0.53484    0.32877   1.627 0.103788    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 884.35  on 682  degrees of freedom\nResidual deviance: 102.89  on 673  degrees of freedom\nAIC: 122.89\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nCall:  glm(formula = benormal ~ ., family = binomial, data = df_clean)\n\nCoefficients:\n     (Intercept)    clumpthickness   uniformcellsize  uniformcellshape  \n       -10.10394           0.53501          -0.00628           0.32271  \n    margadhesion        epithelial        barenuclei    blandchromatin  \n         0.33064           0.09664           0.38302           0.44719  \n  normalnucleoli           mitoses  \n         0.21303           0.53484  \n\nDegrees of Freedom: 682 Total (i.e. Null);  673 Residual\nNull Deviance:      884.4 \nResidual Deviance: 102.9    AIC: 122.9\n\n\nIn this model the variables clumpthickness, margadhesion, barenuclei, blandchromatin were considered the only variables had a significant impact on the response variables with p-values less than .05. The overall model had an AIC of 122.89.\nNext, we wanted to create a new model after logging our variables.\n\n\n\nCall:\nglm(formula = benormal ~ ., family = binomial, data = df_log)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -9.31304    1.25953  -7.394 1.42e-13 ***\nclumpthickness    1.80864    0.59805   3.024  0.00249 ** \nuniformcellsize   0.50056    0.66380   0.754  0.45080    \nuniformcellshape  1.26038    0.74379   1.695  0.09016 .  \nmargadhesion      0.71750    0.39504   1.816  0.06933 .  \nepithelial       -0.04541    0.63630  -0.071  0.94310    \nbarenuclei        0.36117    0.09149   3.948 7.89e-05 ***\nblandchromatin    1.49565    0.61321   2.439  0.01473 *  \nnormalnucleoli    0.48867    0.35560   1.374  0.16938    \nmitoses           1.34541    0.74040   1.817  0.06920 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 884.35  on 682  degrees of freedom\nResidual deviance: 107.02  on 673  degrees of freedom\nAIC: 127.02\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nCall:  glm(formula = benormal ~ ., family = binomial, data = df_log)\n\nCoefficients:\n     (Intercept)    clumpthickness   uniformcellsize  uniformcellshape  \n        -9.31304           1.80864           0.50056           1.26038  \n    margadhesion        epithelial        barenuclei    blandchromatin  \n         0.71750          -0.04541           0.36117           1.49565  \n  normalnucleoli           mitoses  \n         0.48867           1.34541  \n\nDegrees of Freedom: 682 Total (i.e. Null);  673 Residual\nNull Deviance:      884.4 \nResidual Deviance: 107  AIC: 127\n\n\nFor this model the variables clumpthickness and barenuclei were considered the only variables had a significant impact on the response variables with p-values less than .05. The overall model had an AIC of 127.02.\nBased on the AIC of these two models, the non-logged model performed better. Now lets try and simplify the model.\n\n\n\nCall:\nglm(formula = benormal ~ clumpthickness + margadhesion + barenuclei + \n    blandchromatin, family = binomial, data = df_clean)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -10.11370    1.03264  -9.794  &lt; 2e-16 ***\nclumpthickness   0.81166    0.12585   6.450 1.12e-10 ***\nmargadhesion     0.43412    0.11403   3.807 0.000141 ***\nbarenuclei       0.48136    0.08816   5.460 4.76e-08 ***\nblandchromatin   0.70154    0.15196   4.616 3.90e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 884.35  on 682  degrees of freedom\nResidual deviance: 125.77  on 678  degrees of freedom\nAIC: 135.77\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nCall:  glm(formula = benormal ~ clumpthickness + margadhesion + barenuclei + \n    blandchromatin, family = binomial, data = df_clean)\n\nCoefficients:\n   (Intercept)  clumpthickness    margadhesion      barenuclei  blandchromatin  \n      -10.1137          0.8117          0.4341          0.4814          0.7015  \n\nDegrees of Freedom: 682 Total (i.e. Null);  678 Residual\nNull Deviance:      884.4 \nResidual Deviance: 125.8    AIC: 135.8\n\n\nAnalysis of Deviance Table\n\nModel 1: benormal ~ clumpthickness + uniformcellsize + uniformcellshape + \n    margadhesion + epithelial + barenuclei + blandchromatin + \n    normalnucleoli + mitoses\nModel 2: benormal ~ clumpthickness + margadhesion + barenuclei + blandchromatin\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       673     102.89                          \n2       678     125.78 -5  -22.886 0.0003549 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs should be expected, creating a model using only the significant variables was worse at explaining the dataset. Using the Chisq test, we can also conclude that the more complex model is significantly better than the simpler model.\nUsing model_1 as the final model, lets see how accurate it is for prediction.\n\n\n          1           2           3           4           5           6 \n0.016046581 0.908808622 0.008137623 0.760934919 0.018166848 0.999973622 \n          7           8           9          10 \n0.056844170 0.004503358 0.011249056 0.006032371 \n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 434  11\n         1  10 228\n                                          \n               Accuracy : 0.9693          \n                 95% CI : (0.9534, 0.9809)\n    No Information Rate : 0.6501          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.9324          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.9775          \n            Specificity : 0.9540          \n         Pos Pred Value : 0.9753          \n         Neg Pred Value : 0.9580          \n             Prevalence : 0.6501          \n         Detection Rate : 0.6354          \n   Detection Prevalence : 0.6515          \n      Balanced Accuracy : 0.9657          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nLooking at the confusion matrix this is a very good model with high predictability for both false positives and negatives. The % chance the model falsely predicted cancer when not was 10/444 (2.2%) and the chance the model falsely predicted not having cancer when there was 11/239 (4.6%).\n\n\nDecision Tree and Random Forest Modeling\nAnother modeling technique we used for predicting whether a cell was cancerous or not was a random forest model. One of the biggest advantages of random forests is its versatility. It can be used for both regression and classification tasks, and it’s also easy to view the relative importance it assigns to the input features. One of the biggest problems in machine learning is overfitting, but most of the time this won’t happen thanks to the random forest classifier. If there are enough trees in the forest, the classifier won’t overfit the model.\nFirst, we looked at a decision tree model to determine the best spilt for node splitting. We did this by splitting the data into two sets, training and testing to train the model and then test its accuracy.\nFor this first model we included all of the variables in the dataset.\n\n\n\n\n\n\n\n\n\n\nRegression tree:\ntree(formula = benormal ~ ., data = df_clean, subset = train)\nVariables actually used in tree construction:\n[1] \"uniformcellshape\" \"clumpthickness\"   \"barenuclei\"       \"epithelial\"      \nNumber of terminal nodes:  5 \nResidual mean deviance:  0.02505 = 8.416 / 336 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.95240 -0.00495 -0.00495  0.00000  0.04762  0.99500 \n\n\nBased on this model alone we have a very strong model with a mean squared error (MSE) of just .025.\nWe then wanted to see if we could make the model better by keeping the residual difference low by pruning the tree and finding the lowest number of nodes to create a split with. Looking at the cross-validated decision tree graph below it looks like splitting the tree using 3 nodes is best.\n\n\n\n\n\n\n\n\n\nAfter pruning the tree and using 3 nodes to spilt the data below is the resulting model.\n\n\n\nRegression tree:\nsnip.tree(tree = tree.df_clean, nodes = c(2L, 6L))\nVariables actually used in tree construction:\n[1] \"uniformcellshape\" \"barenuclei\"      \nNumber of terminal nodes:  3 \nResidual mean deviance:  0.04432 = 14.98 / 338 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.95240 -0.01914 -0.01914  0.00000  0.04762  0.98090 \n\n\n\n\n\n\n\n\n\nThis is another strong model with a MSE of just .039. When comparing these models, their resdidual variances are very similar, as such we would likely choose the more simple model.\nLets now use a random forest model to prevent any potential overfitting of our decision tree model.\n\n\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n\n\n[1] 0.02856504\n\n\nBy creating a random forest model we were able to reduce the MSE from .039 to .026\n\n\n\n\n\n\n\n\n\nTaking a look a which variables were most important in the model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe variable that is most important to reduce the mean standard error (MSE) is barenuceli and the variables that were deemed the most important to include in the node splitting were uniformcellsize and uniformcellshape. Overall, the model is very good with 88.7% of the variables explained. Looking at the confusion matrix:\n\n\n            1             2             3             4             5 \n-6.231682e-16  7.668333e-01 -6.217249e-16  3.937667e-01  1.412692e-02 \n            6             7             8             9            10 \n 9.996000e-01  2.192000e-01 -6.310508e-16  1.061333e-01  7.692308e-05 \n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 438  10\n         1   6 229\n                                          \n               Accuracy : 0.9766          \n                 95% CI : (0.9622, 0.9866)\n    No Information Rate : 0.6501          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.9483          \n                                          \n Mcnemar's Test P-Value : 0.4533          \n                                          \n            Sensitivity : 0.9865          \n            Specificity : 0.9582          \n         Pos Pred Value : 0.9777          \n         Neg Pred Value : 0.9745          \n             Prevalence : 0.6501          \n         Detection Rate : 0.6413          \n   Detection Prevalence : 0.6559          \n      Balanced Accuracy : 0.9723          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nThe percent chance the model falsely predicted cancer when not was 6/444 (1.4%) and the chance the model falsely predicted not having cancer when there was 9/239 (3.7%)."
  },
  {
    "objectID": "Final_Report_Webpage.html#iii.-limitations",
    "href": "Final_Report_Webpage.html#iii.-limitations",
    "title": "Final Report",
    "section": "iii. Limitations",
    "text": "iii. Limitations\nThere were no major limitations for this analysis but there were a few of things that needed to be done to the dataset in order to clean and make the dataset usable for analysis. First, we had to identify and remove all missing rows from the dataset. Second, we had to transform the response variable to be “0” and “1”. Finally, many of the variables seemed to show a left skew making us question if the variables should be transformed or not. \nThere were only 699 samples for the dataset and although a solid number of samples, more samples would lead to a more predictable conclusion."
  },
  {
    "objectID": "Final_Report_Webpage.html#iv.-conclusion",
    "href": "Final_Report_Webpage.html#iv.-conclusion",
    "title": "Final Report",
    "section": "iv. Conclusion",
    "text": "iv. Conclusion\nIt seems that a significantly accurate model can be created to predict if a cancerous cell could be malignant using the measurements recorded in this dataset. To test this, we created multiple regression models, a random forest model, and a PCA analysis to understand the variables more thoroughly.  \nIn the PCA test we determined that the dataset’s variance could be explained by simplifying and using only 2 of 9 predictor variables. We then created multiple logistic regression models and compared them to each other to choose the best one. After variable transformation and selection, we determined the best model would be to use all the variables. The percent chance the model falsely predicted cancer when not was 10/444 (2.2%) and the chance the model falsely predicted not having cancer when there was 11/239 (4.6%). We then decided to try and create an optimal best random forest model, starting with a best fit decision tree model. The created model resulted in an overall accuracy rate of almost 98% with the percent chance the model falsely predicted cancer when not was 6/444 (1.4%) and the chance the model falsely predicted not having cancer when there was 9/239 (3.7%). Based on the two models we created, the best model to use would be the random forest model because of its high accuracy and predictability."
  },
  {
    "objectID": "Final_Report_Webpage.html#references",
    "href": "Final_Report_Webpage.html#references",
    "title": "Final Report",
    "section": "References",
    "text": "References\n[1] Wolberg,WIlliam. (1992). Breast Cancer Wisconsin (Original). UCI Machine Learning Repository. https://doi.org/10.24432/C5HP4Z. \n\n\n  - Grolemund G, Wickham H (2011). \"Dates and Times Made Easy with lubridate.\" _Journal of Statistical Software_, *40*(3), 1-25. &lt;https://www.jstatsoft.org/v40/i03/&gt;.\n  - James G, Witten D, Hastie T, Tibshirani R (2021). _ISLR: Data for an Introduction to Statistical Learning with Applications in R_. R package version 1.4, &lt;https://CRAN.R-project.org/package=ISLR&gt;.\n  - Kuhn, Max (2008). \"Building Predictive Models in R Using the caret Package.\" _Journal of Statistical Software_, *28*(5), 1–26. doi:10.18637/jss.v028.i05 &lt;https://doi.org/10.18637/jss.v028.i05&gt;, &lt;https://www.jstatsoft.org/index.php/jss/article/view/v028i05&gt;.\n  - Liaw A, Wiener M (2002). \"Classification and Regression by randomForest.\" _R News_, *2*(3), 18-22. &lt;https://CRAN.R-project.org/doc/Rnews/&gt;.\n  - Makowski D, Lüdecke D, Patil I, Thériault R, Ben-Shachar M, Wiernik B (2023). \"Automated Results Reporting as a Practical Tool to Improve Reproducibility and Methodological Best Practices Adoption.\" _CRAN_. &lt;https://easystats.github.io/report/&gt;.\n  - Miller TLboFcbA (2020). _leaps: Regression Subset Selection_. R package version 3.1, &lt;https://CRAN.R-project.org/package=leaps&gt;.\n  - Müller K, Wickham H (2023). _tibble: Simple Data Frames_. R package version 3.2.1, &lt;https://CRAN.R-project.org/package=tibble&gt;.\n  - R Core Team (2023). _R: A Language and Environment for Statistical Computing_. R Foundation for Statistical Computing, Vienna, Austria. &lt;https://www.R-project.org/&gt;.\n  - Ripley B (2023). _tree: Classification and Regression Trees_. R package version 1.0-43, &lt;https://CRAN.R-project.org/package=tree&gt;.\n  - Sarkar D (2008). _Lattice: Multivariate Data Visualization with R_. Springer, New York. ISBN 978-0-387-75968-5, &lt;http://lmdvr.r-forge.r-project.org&gt;.\n  - Sievert C (2020). _Interactive Web-Based Data Visualization with R, plotly, and shiny_. Chapman and Hall/CRC. ISBN 9781138331457, &lt;https://plotly-r.com&gt;.\n  - Wickham H (2007). \"Reshaping Data with the reshape Package.\" _Journal of Statistical Software_, *21*(12), 1-20. &lt;http://www.jstatsoft.org/v21/i12/&gt;.\n  - Wickham H (2016). _ggplot2: Elegant Graphics for Data Analysis_. Springer-Verlag New York. ISBN 978-3-319-24277-4, &lt;https://ggplot2.tidyverse.org&gt;.\n  - Wickham H (2022). _stringr: Simple, Consistent Wrappers for Common String Operations_. R package version 1.5.0, &lt;https://CRAN.R-project.org/package=stringr&gt;.\n  - Wickham H (2023). _forcats: Tools for Working with Categorical Variables (Factors)_. R package version 1.0.0, &lt;https://CRAN.R-project.org/package=forcats&gt;.\n  - Wickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). \"Welcome to the tidyverse.\" _Journal of Open Source Software_, *4*(43), 1686. doi:10.21105/joss.01686 &lt;https://doi.org/10.21105/joss.01686&gt;.\n  - Wickham H, François R, Henry L, Müller K, Vaughan D (2023). _dplyr: A Grammar of Data Manipulation_. R package version 1.1.3, &lt;https://CRAN.R-project.org/package=dplyr&gt;.\n  - Wickham H, Henry L (2023). _purrr: Functional Programming Tools_. R package version 1.0.2, &lt;https://CRAN.R-project.org/package=purrr&gt;.\n  - Wickham H, Hester J, Bryan J (2023). _readr: Read Rectangular Text Data_. R package version 2.1.4, &lt;https://CRAN.R-project.org/package=readr&gt;.\n  - Wickham H, Vaughan D, Girlich M (2023). _tidyr: Tidy Messy Data_. R package version 1.3.0, &lt;https://CRAN.R-project.org/package=tidyr&gt;.\n  - William Revelle (2024). _psych: Procedures for Psychological, Psychometric, and Personality Research_. Northwestern University, Evanston, Illinois. R package version 2.4.3, &lt;https://CRAN.R-project.org/package=psych&gt;."
  }
]